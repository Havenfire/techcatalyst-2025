{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1c54f92",
   "metadata": {},
   "source": [
    "# Load ENV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6dc39a9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install boto3 python-dotenv -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef8fa0c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b484de6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "aws_access_key_id = os.getenv('AWS_ACCESS_KEY_ID')\n",
    "aws_secret_access_key = os.getenv('AWS_SECRET_ACCESS_KEY')\n",
    "aws_region = os.getenv('AWS_DEFAULT_REGION')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0a75ac",
   "metadata": {},
   "source": [
    "# Lab 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1169464f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "s3_client = boto3.client(\n",
    "    's3',\n",
    "    aws_access_key_id=aws_access_key_id,\n",
    "    aws_secret_access_key=aws_secret_access_key,\n",
    "    region_name=aws_region\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f75ec3f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "capstone-techcatalyst-conformed\n",
      "capstone-techcatalyst-raw\n",
      "capstone-techcatalyst-transformed\n",
      "techcatalyst-public\n",
      "techcatalyst-raw\n",
      "techcatalyst-transformed\n"
     ]
    }
   ],
   "source": [
    "# list bucket names\n",
    "buckets = s3_client.list_buckets()\n",
    "for bucket in buckets['Buckets']:\n",
    "    if 'techcatalyst' in bucket['Name']:\n",
    "        print(bucket['Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d63dcbe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLAKE/test_export.parquet\n",
      "BLAKE/upload_file_method_GOOG.csv\n",
      "BLAKE/upload_fileobj_method.txt\n",
      "BLAKE_wr/9288e6c1eed4476d98eade7875cbf9c0.snappy.parquet\n",
      "BLAKE_wr/uploads/wr_newfile.csv\n",
      "Ben/Million_Songs/\n",
      "Ben/bingchilling.txt\n",
      "Ben/gooooog.csv\n",
      "Ben/parquetGoogleStock/a0eeb97dbd854a5c9cc6e89512faee73.snappy.parquet\n",
      "Ben/test.csv\n",
      "Ben/test2.csv\n",
      "Ben/uploads/Google_upload_test\n",
      "EMMA/emna_goog.csv\n",
      "EMMA/emna_goog.txt\n",
      "EMMA/test_export.parquet\n",
      "MELISSA/test.csv\n",
      "MELISSA/uploads/GOOG.csv\n",
      "MELISSA/uploads/fileobj.txt\n",
      "MELISSA/uploads/new_file.csv\n",
      "MillionSongSubset/\n",
      "MillionSongSubset/log-data/2018-11-01-events.json\n",
      "MillionSongSubset/log-data/2018-11-02-events.json\n",
      "MillionSongSubset/log-data/2018-11-03-events.json\n",
      "MillionSongSubset/log-data/2018-11-04-events.json\n",
      "MillionSongSubset/log-data/2018-11-05-events.json\n",
      "MillionSongSubset/log-data/2018-11-06-events.json\n",
      "MillionSongSubset/log-data/2018-11-07-events.json\n",
      "MillionSongSubset/log-data/2018-11-08-events.json\n",
      "MillionSongSubset/log-data/2018-11-09-events.json\n",
      "MillionSongSubset/log-data/2018-11-10-events.json\n",
      "MillionSongSubset/log-data/2018-11-11-events.json\n",
      "MillionSongSubset/log-data/2018-11-12-events.json\n",
      "MillionSongSubset/log-data/2018-11-13-events.json\n",
      "MillionSongSubset/log-data/2018-11-14-events.json\n",
      "MillionSongSubset/log-data/2018-11-15-events.json\n",
      "MillionSongSubset/log-data/2018-11-16-events.json\n",
      "MillionSongSubset/log-data/2018-11-17-events.json\n",
      "MillionSongSubset/log-data/2018-11-18-events.json\n",
      "MillionSongSubset/log-data/2018-11-19-events.json\n",
      "MillionSongSubset/log-data/2018-11-20-events.json\n",
      "MillionSongSubset/log-data/2018-11-21-events.json\n",
      "MillionSongSubset/log-data/2018-11-22-events.json\n",
      "MillionSongSubset/log-data/2018-11-23-events.json\n",
      "MillionSongSubset/log-data/2018-11-24-events.json\n",
      "MillionSongSubset/log-data/2018-11-25-events.json\n",
      "MillionSongSubset/log-data/2018-11-26-events.json\n",
      "MillionSongSubset/log-data/2018-11-27-events.json\n",
      "MillionSongSubset/log-data/2018-11-28-events.json\n",
      "MillionSongSubset/log-data/2018-11-29-events.json\n",
      "MillionSongSubset/log-data/2018-11-30-events.json\n",
      "MillionSongSubset/song_data/A/A/A/TRAAAAW128F429D538.json\n",
      "MillionSongSubset/song_data/A/A/A/TRAAABD128F429CF47.json\n",
      "MillionSongSubset/song_data/A/A/A/TRAAADZ128F9348C2E.json\n",
      "MillionSongSubset/song_data/A/A/A/TRAAAEF128F4273421.json\n",
      "MillionSongSubset/song_data/A/A/A/TRAAAFD128F92F423A.json\n",
      "MillionSongSubset/song_data/A/A/A/TRAAAMO128F1481E7F.json\n",
      "MillionSongSubset/song_data/A/A/A/TRAAAMQ128F1460CD3.json\n",
      "MillionSongSubset/song_data/A/A/A/TRAAAPK128E0786D96.json\n",
      "MillionSongSubset/song_data/A/A/A/TRAAARJ128F9320760.json\n",
      "MillionSongSubset/song_data/A/A/A/TRAAAVG12903CFA543.json\n",
      "MillionSongSubset/song_data/A/A/A/TRAAAVO128F93133D4.json\n",
      "MillionSongSubset/song_data/A/A/B/TRAABCL128F4286650.json\n",
      "MillionSongSubset/song_data/A/A/B/TRAABDL12903CAABBA.json\n",
      "MillionSongSubset/song_data/A/A/B/TRAABJL12903CDCF1A.json\n",
      "MillionSongSubset/song_data/A/A/B/TRAABJV128F1460C49.json\n",
      "MillionSongSubset/song_data/A/A/B/TRAABLR128F423B7E3.json\n",
      "MillionSongSubset/song_data/A/A/B/TRAABNV128F425CEE1.json\n",
      "MillionSongSubset/song_data/A/A/B/TRAABRB128F9306DD5.json\n",
      "MillionSongSubset/song_data/A/A/B/TRAABVM128F92CA9DC.json\n",
      "MillionSongSubset/song_data/A/A/B/TRAABXG128F9318EBD.json\n",
      "MillionSongSubset/song_data/A/A/B/TRAABYN12903CFD305.json\n",
      "MillionSongSubset/song_data/A/A/B/TRAABYW128F4244559.json\n",
      "MillionSongSubset/song_data/A/A/C/TRAACCG128F92E8A55.json\n",
      "MillionSongSubset/song_data/A/A/C/TRAACER128F4290F96.json\n",
      "MillionSongSubset/song_data/A/A/C/TRAACFV128F935E50B.json\n",
      "MillionSongSubset/song_data/A/A/C/TRAACHN128F1489601.json\n",
      "MillionSongSubset/song_data/A/A/C/TRAACIW12903CC0F6D.json\n",
      "MillionSongSubset/song_data/A/A/C/TRAACLV128F427E123.json\n",
      "MillionSongSubset/song_data/A/A/C/TRAACNS128F14A2DF5.json\n",
      "MillionSongSubset/song_data/A/A/C/TRAACOW128F933E35F.json\n",
      "MillionSongSubset/song_data/A/A/C/TRAACPE128F421C1B9.json\n",
      "MillionSongSubset/song_data/A/A/C/TRAACQT128F9331780.json\n",
      "MillionSongSubset/song_data/A/A/C/TRAACSL128F93462F4.json\n",
      "MillionSongSubset/song_data/A/A/C/TRAACTB12903CAAF15.json\n",
      "MillionSongSubset/song_data/A/A/C/TRAACVS128E078BE39.json\n",
      "MillionSongSubset/song_data/A/A/C/TRAACZK128F4243829.json\n",
      "MillionSongSubset/song_data/A/B/A/TRABACN128F425B784.json\n",
      "MillionSongSubset/song_data/A/B/A/TRABAFJ128F42AF24E.json\n",
      "MillionSongSubset/song_data/A/B/A/TRABAFP128F931E9A1.json\n",
      "MillionSongSubset/song_data/A/B/A/TRABAIO128F42938F9.json\n",
      "MillionSongSubset/song_data/A/B/A/TRABATO128F42627E9.json\n",
      "MillionSongSubset/song_data/A/B/A/TRABAVQ12903CBF7E0.json\n",
      "MillionSongSubset/song_data/A/B/A/TRABAWW128F4250A31.json\n",
      "MillionSongSubset/song_data/A/B/A/TRABAXL128F424FC50.json\n",
      "MillionSongSubset/song_data/A/B/A/TRABAXR128F426515F.json\n",
      "MillionSongSubset/song_data/A/B/A/TRABAXV128F92F6AE3.json\n",
      "MillionSongSubset/song_data/A/B/A/TRABAZH128F930419A.json\n",
      "MillionSongSubset/song_data/A/B/B/TRABBAM128F429D223.json\n",
      "MillionSongSubset/song_data/A/B/B/TRABBBV128F42967D7.json\n",
      "MillionSongSubset/song_data/A/B/B/TRABBJE12903CDB442.json\n",
      "MillionSongSubset/song_data/A/B/B/TRABBKX128F4285205.json\n",
      "MillionSongSubset/song_data/A/B/B/TRABBLU128F93349CF.json\n",
      "MillionSongSubset/song_data/A/B/B/TRABBNP128F932546F.json\n",
      "MillionSongSubset/song_data/A/B/B/TRABBOP128F931B50D.json\n",
      "MillionSongSubset/song_data/A/B/B/TRABBOR128F4286200.json\n",
      "MillionSongSubset/song_data/A/B/B/TRABBTA128F933D304.json\n",
      "MillionSongSubset/song_data/A/B/B/TRABBVJ128F92F7EAA.json\n",
      "MillionSongSubset/song_data/A/B/B/TRABBXU128F92FEF48.json\n",
      "MillionSongSubset/song_data/A/B/B/TRABBZN12903CD9297.json\n",
      "MillionSongSubset/song_data/A/B/C/TRABCAJ12903CDFCC2.json\n",
      "MillionSongSubset/song_data/A/B/C/TRABCEC128F426456E.json\n",
      "MillionSongSubset/song_data/A/B/C/TRABCEI128F424C983.json\n",
      "MillionSongSubset/song_data/A/B/C/TRABCFL128F149BB0D.json\n",
      "MillionSongSubset/song_data/A/B/C/TRABCIX128F4265903.json\n",
      "MillionSongSubset/song_data/A/B/C/TRABCKL128F423A778.json\n",
      "MillionSongSubset/song_data/A/B/C/TRABCPZ128F4275C32.json\n",
      "MillionSongSubset/song_data/A/B/C/TRABCRU128F423F449.json\n",
      "MillionSongSubset/song_data/A/B/C/TRABCTK128F934B224.json\n",
      "MillionSongSubset/song_data/A/B/C/TRABCUQ128E0783E2B.json\n",
      "MillionSongSubset/song_data/A/B/C/TRABCXB128F4286BD3.json\n",
      "MillionSongSubset/song_data/A/B/C/TRABCYE128F934CE1D.json\n",
      "YOURNAME/test_export.parquet\n",
      "aamnah/33633ec108fb4dfdaa3d078cda6d1e0b_000000_000000.snappy.parquet\n",
      "aamnah/33633ec108fb4dfdaa3d078cda6d1e0b_000001_000000.snappy.parquet\n",
      "aamnah/33633ec108fb4dfdaa3d078cda6d1e0b_000002_000000.snappy.parquet\n",
      "aamnah/33633ec108fb4dfdaa3d078cda6d1e0b_000003_000000.snappy.parquet\n",
      "accidents/\n",
      "accidents/accidents_2017_to_2023_english.csv\n",
      "alexia/957133637c684b09a94cc753e0c594a2_000000_000000.snappy.parquet\n",
      "alexia/957133637c684b09a94cc753e0c594a2_000001_000000.snappy.parquet\n",
      "alexia/957133637c684b09a94cc753e0c594a2_000002_000000.snappy.parquet\n",
      "alexia/957133637c684b09a94cc753e0c594a2_000003_000000.snappy.parquet\n",
      "alexia/secret_formula.txt\n",
      "alexia/upload_GOOG.csv\n",
      "emma/3a1b7394faa84991a5e7700c963af73f_000000_000000.snappy.parquet\n",
      "emma/3a1b7394faa84991a5e7700c963af73f_000001_000000.snappy.parquet\n",
      "emma/3a1b7394faa84991a5e7700c963af73f_000002_000000.snappy.parquet\n",
      "emma/3a1b7394faa84991a5e7700c963af73f_000003_000000.snappy.parquet\n",
      "emma/uploads/wr_emma_stock\n",
      "fabiola/a3a03886216d40279ccfca68cc237524.snappy.parquet\n",
      "fabiola/test.csv\n",
      "fabiola/uploads/wr_GOOG.csv\n",
      "jaden/6e08eee074434aa7abda59ab22bc3ea7.snappy.parquet\n",
      "jaden/test.csv\n",
      "jaden/uploads/new_file.csv\n",
      "michael/49d5f54d6c4e46b8b4e01f07f7861c35.snappy.parquet\n",
      "michael/uploads/new_file_stocks.csv\n",
      "miraj/b4ec7d55de664e65992d4ae03886f3ff.snappy.parquet\n",
      "miraj_jara/uploads/google_stock_data\n",
      "shaswat/e4e70d26665c4d07af4bdaab85f8975e.snappy.parquet\n",
      "shaswat/test.csv\n",
      "shaswat/uploads/new_file.csv\n",
      "stage/\n",
      "stage/yellow_tripdata.csv\n",
      "stage/yellow_tripdata.json\n",
      "stage/yellow_tripdata.parquet\n",
      "stocks/\n",
      "stocks/GOOG.csv\n",
      "suchitha/19027023cb9a4746b2e915ff13905e66.snappy.parquet\n",
      "suchitha/uploads/sales.csv\n",
      "tatwan/51addc9167684aa281c96fd5ae2d5be2.snappy.parquet\n",
      "tatwan/GOOG.csv\n",
      "tatwan/GOOG_NEW.csv\n",
      "tatwan/buffer.txt\n",
      "taxi_data/\n",
      "taxi_data/8d12668ed190477fa03a29a0a5d50e84_000000_000000.snappy.parquet\n",
      "taxi_data/8d12668ed190477fa03a29a0a5d50e84_000001_000000.snappy.parquet\n",
      "taxi_data/8d12668ed190477fa03a29a0a5d50e84_000002_000000.snappy.parquet\n",
      "taxi_data/8d12668ed190477fa03a29a0a5d50e84_000003_000000.snappy.parquet\n",
      "taxi_data/yellow_tripdata_2024-01.parquet\n",
      "taxi_data/yellow_tripdata_2024-02.parquet\n",
      "taxi_data/yellow_tripdata_2024-03.parquet\n",
      "taxi_data/yellow_tripdata_2024-04.parquet\n",
      "tyler/273176db78374c948307fa0c28b7d6b9.snappy.parquet\n",
      "tyler/uploads/goog_stock_file.csv\n"
     ]
    }
   ],
   "source": [
    "# list objects in a specific bucket \"techcatalyst-raw\" \n",
    "bucket_name = \"techcatalyst-raw\"\n",
    "objects = s3_client.list_objects_v2(Bucket=bucket_name)\n",
    "for obj in objects.get('Contents', []):\n",
    "    print(obj['Key'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13e9467d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLAKE/upload_file_method_GOOG.csv\n",
      "BLAKE_wr/uploads/wr_newfile.csv\n",
      "Ben/gooooog.csv\n",
      "Ben/test.csv\n",
      "Ben/test2.csv\n",
      "EMMA/emna_goog.csv\n",
      "MELISSA/test.csv\n",
      "MELISSA/uploads/GOOG.csv\n",
      "MELISSA/uploads/new_file.csv\n",
      "accidents/accidents_2017_to_2023_english.csv\n",
      "alexia/upload_GOOG.csv\n",
      "fabiola/test.csv\n",
      "fabiola/uploads/wr_GOOG.csv\n",
      "jaden/test.csv\n",
      "jaden/uploads/new_file.csv\n",
      "michael/uploads/new_file_stocks.csv\n",
      "shaswat/test.csv\n",
      "shaswat/uploads/new_file.csv\n",
      "stage/yellow_tripdata.csv\n",
      "stocks/GOOG.csv\n",
      "suchitha/uploads/sales.csv\n",
      "tatwan/GOOG.csv\n",
      "tatwan/GOOG_NEW.csv\n",
      "tyler/uploads/goog_stock_file.csv\n"
     ]
    }
   ],
   "source": [
    "# list objects that are CSV in a specific bucket \"techcatalyst-raw\" \n",
    "for obj in objects.get('Contents', []):\n",
    "    output =  obj.get('Key').split('.')\n",
    "    if len(output) > 1:\n",
    "        if output[1] == \"csv\":\n",
    "            print(obj.get('Key'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "594c3fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client.download_file(Bucket='techcatalyst-raw',  # from which bucket\n",
    "                        Key='stocks/GOOG.csv',\n",
    "                        Filename='GOOG.csv') # Filename is what you want to call it once it is downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "588c5860",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "io_temp = io.BytesIO()\n",
    "temp = s3_client.download_fileobj(Bucket='techcatalyst-raw',  # from which bucket\n",
    "                                    Key='stocks/GOOG.csv',\n",
    "                            \t\tFileobj=io_temp) # pass the io.BytesIO object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4d9816c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Date,Open,High,Low,Close,Volume\\r\\n1/2/2025 16:00:00,191.49,193.2,188.71,190.63,17545162\\r\\n1/3/2025 16:'\n"
     ]
    }
   ],
   "source": [
    "print(io_temp.getvalue()[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5998508e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "io_temp.seek(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9424b706",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'Date,Open,High,Low,Close,Volume\\r\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(io_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "352be9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "io_temp.seek(0)\n",
    "with open('google_stock_downloaded.csv', 'wb') as f:\n",
    "    f.write(io_temp.getvalue())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "69976206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uploading a local file using upload_file\n",
    "s3_client.upload_file(Filename='GOOG.csv', # local file name\n",
    "                      Bucket='techcatalyst-raw', # the bucket target\n",
    "                      Key='BLAKE/upload_file_method_GOOG.csv') # destination name, make sure it include YOURNAME/ANY_FILE_NAME.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "09c103b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_memory_file = io.BytesIO(b\"Smash your competition, baby\\nShow us some good entertainment\")\n",
    "s3_client.upload_fileobj(Fileobj=in_memory_file,\n",
    "                          Bucket='techcatalyst-raw', \n",
    "                          Key='BLAKE/upload_fileobj_method.txt') # destination name, make sure it include YOURNAME/ANY_FILE_NAME.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "94fb1d12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Key': 'BLAKE/test_export.parquet',\n",
       "  'LastModified': datetime.datetime(2025, 7, 25, 15, 25, 44, tzinfo=tzlocal()),\n",
       "  'ETag': '\"3c8cff84500ad4c4c85361e38dce05ed-1\"',\n",
       "  'ChecksumAlgorithm': ['CRC64NVME'],\n",
       "  'ChecksumType': 'FULL_OBJECT',\n",
       "  'Size': 8393,\n",
       "  'StorageClass': 'STANDARD'},\n",
       " {'Key': 'BLAKE/upload_file_method_GOOG.csv',\n",
       "  'LastModified': datetime.datetime(2025, 8, 5, 12, 13, 8, tzinfo=tzlocal()),\n",
       "  'ETag': '\"8cbbdc687ee45f1fe58a522e16d423c2\"',\n",
       "  'ChecksumAlgorithm': ['CRC32'],\n",
       "  'ChecksumType': 'FULL_OBJECT',\n",
       "  'Size': 7889,\n",
       "  'StorageClass': 'STANDARD'},\n",
       " {'Key': 'BLAKE/upload_fileobj_method.txt',\n",
       "  'LastModified': datetime.datetime(2025, 8, 5, 12, 13, 8, tzinfo=tzlocal()),\n",
       "  'ETag': '\"7f1c0a498090833fa8c6c54d7d3b3a5a\"',\n",
       "  'ChecksumAlgorithm': ['CRC32'],\n",
       "  'ChecksumType': 'FULL_OBJECT',\n",
       "  'Size': 60,\n",
       "  'StorageClass': 'STANDARD'},\n",
       " {'Key': 'BLAKE_wr/9288e6c1eed4476d98eade7875cbf9c0.snappy.parquet',\n",
       "  'LastModified': datetime.datetime(2025, 8, 4, 19, 37, 40, tzinfo=tzlocal()),\n",
       "  'ETag': '\"d24428517841e34980e5f55aa74a0ed5\"',\n",
       "  'ChecksumAlgorithm': ['CRC32'],\n",
       "  'ChecksumType': 'FULL_OBJECT',\n",
       "  'Size': 9146,\n",
       "  'StorageClass': 'STANDARD'},\n",
       " {'Key': 'BLAKE_wr/uploads/wr_newfile.csv',\n",
       "  'LastModified': datetime.datetime(2025, 8, 4, 19, 39, 28, tzinfo=tzlocal()),\n",
       "  'ETag': '\"8cbbdc687ee45f1fe58a522e16d423c2\"',\n",
       "  'ChecksumAlgorithm': ['CRC32'],\n",
       "  'ChecksumType': 'FULL_OBJECT',\n",
       "  'Size': 7889,\n",
       "  'StorageClass': 'STANDARD'}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "objects = s3_client.list_objects_v2(Bucket='techcatalyst-raw', Prefix='BLAKE')[\"Contents\"]\n",
    "objects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3d219d",
   "metadata": {},
   "source": [
    "# Lab 3: Introduction to AWS Wrangler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "206d898e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "! pip install awswrangler -q\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bd514527",
   "metadata": {},
   "outputs": [],
   "source": [
    "import awswrangler as wr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6015562b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1/2/2025 16:00:00</td>\n",
       "      <td>191.49</td>\n",
       "      <td>193.20</td>\n",
       "      <td>188.71</td>\n",
       "      <td>190.63</td>\n",
       "      <td>17545162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1/3/2025 16:00:00</td>\n",
       "      <td>192.73</td>\n",
       "      <td>194.50</td>\n",
       "      <td>191.35</td>\n",
       "      <td>193.13</td>\n",
       "      <td>12874957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1/6/2025 16:00:00</td>\n",
       "      <td>195.15</td>\n",
       "      <td>199.56</td>\n",
       "      <td>195.06</td>\n",
       "      <td>197.96</td>\n",
       "      <td>19483323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1/7/2025 16:00:00</td>\n",
       "      <td>198.27</td>\n",
       "      <td>202.14</td>\n",
       "      <td>195.94</td>\n",
       "      <td>196.71</td>\n",
       "      <td>16966760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1/8/2025 16:00:00</td>\n",
       "      <td>193.95</td>\n",
       "      <td>197.64</td>\n",
       "      <td>193.75</td>\n",
       "      <td>195.39</td>\n",
       "      <td>14335341</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Date    Open    High     Low   Close    Volume\n",
       "0  1/2/2025 16:00:00  191.49  193.20  188.71  190.63  17545162\n",
       "1  1/3/2025 16:00:00  192.73  194.50  191.35  193.13  12874957\n",
       "2  1/6/2025 16:00:00  195.15  199.56  195.06  197.96  19483323\n",
       "3  1/7/2025 16:00:00  198.27  202.14  195.94  196.71  16966760\n",
       "4  1/8/2025 16:00:00  193.95  197.64  193.75  195.39  14335341"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test if you can read from a private bucket\n",
    "\n",
    "df = wr.s3.read_csv('s3://techcatalyst-raw/stocks/GOOG.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ac6db49a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 140 entries, 0 to 139\n",
      "Data columns (total 6 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   Date    140 non-null    object \n",
      " 1   Open    140 non-null    float64\n",
      " 2   High    140 non-null    float64\n",
      " 3   Low     140 non-null    float64\n",
      " 4   Close   140 non-null    float64\n",
      " 5   Volume  140 non-null    int64  \n",
      "dtypes: float64(4), int64(1), object(1)\n",
      "memory usage: 6.7+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "41f3b5d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Database       Description\n",
      "0             aamnah_db                  \n",
      "1           aamnah_taxi                  \n",
      "2             alexia_db                  \n",
      "3           alexia_logs                  \n",
      "4           alexia_song                  \n",
      "5      awswrangler_test                  \n",
      "6                ben_db                  \n",
      "7              ben_song                  \n",
      "8              ben_taxi                  \n",
      "9            blake_taxi                  \n",
      "10          blake_wr_db                  \n",
      "11              default  default database\n",
      "12              emma_db                  \n",
      "13            emma_taxi                  \n",
      "14           fabiola_db                  \n",
      "15         fabiola_taxi                  \n",
      "16           jaden_taxi                  \n",
      "17        jadenastle_db                  \n",
      "18           melissa_db                  \n",
      "19         melissa_logs                  \n",
      "20        melissa_songs                  \n",
      "21         melissa_taxi                  \n",
      "22           michael_db                  \n",
      "23             miraj_db                  \n",
      "24           miraj_taxi                  \n",
      "25                my_db                  \n",
      "26           shaswat_db                  \n",
      "27         shaswat_logs                  \n",
      "28         shaswat_song                  \n",
      "29         shaswat_taxi                  \n",
      "30          suchitha_db                  \n",
      "31        suchitha_taxi                  \n",
      "32            tatwan_db                  \n",
      "33  tatwan_inclass_demo                  \n",
      "34          tatwan_taxi                  \n",
      "35             tyler_db                  \n",
      "36           tyler_taxi                  \n"
     ]
    }
   ],
   "source": [
    "databases = wr.catalog.databases()\n",
    "print(databases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6a679e52",
   "metadata": {},
   "outputs": [
    {
     "ename": "AlreadyExists",
     "evalue": "Database BLAKE_wr_db already exists and <exist_ok> is set to False.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAlreadyExists\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m name = \u001b[33m\"\u001b[39m\u001b[33mBLAKE_wr\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      2\u001b[39m database_name = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_db\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mwr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcatalog\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_database\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdatabase_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/awswrangler/_config.py:712\u001b[39m, in \u001b[36mapply_configs.<locals>.wrapper\u001b[39m\u001b[34m(*args_raw, **kwargs)\u001b[39m\n\u001b[32m    710\u001b[39m         \u001b[38;5;28;01mdel\u001b[39;00m args[name]\n\u001b[32m    711\u001b[39m         args = {**args, **keywords}\n\u001b[32m--> \u001b[39m\u001b[32m712\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/awswrangler/catalog/_create.py:712\u001b[39m, in \u001b[36mcreate_database\u001b[39m\u001b[34m(name, description, catalog_id, exist_ok, database_input_args, boto3_session)\u001b[39m\n\u001b[32m    710\u001b[39m r = client_glue.get_database(**_catalog_id(catalog_id=catalog_id, Name=name))\n\u001b[32m    711\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m exist_ok:\n\u001b[32m--> \u001b[39m\u001b[32m712\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions.AlreadyExists(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDatabase \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m already exists and <exist_ok> is set to False.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    713\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m args.items():\n\u001b[32m    714\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m v != r[\u001b[33m\"\u001b[39m\u001b[33mDatabase\u001b[39m\u001b[33m\"\u001b[39m].get(k, \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[31mAlreadyExists\u001b[39m: Database BLAKE_wr_db already exists and <exist_ok> is set to False."
     ]
    }
   ],
   "source": [
    "name = \"BLAKE_wr\"\n",
    "database_name = f\"{name}_db\"\n",
    "wr.catalog.create_database(database_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248eb6c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Database       Description\n",
      "0           alexia_logs                  \n",
      "1           alexia_song                  \n",
      "2      awswrangler_test                  \n",
      "3              ben_song                  \n",
      "4           blake_wr_db                  \n",
      "5               default  default database\n",
      "6               emma_db                  \n",
      "7         jadenastle_db                  \n",
      "8          melissa_logs                  \n",
      "9         melissa_songs                  \n",
      "10           michael_db                  \n",
      "11                my_db                  \n",
      "12         shaswat_logs                  \n",
      "13         shaswat_song                  \n",
      "14            tatwan_db                  \n",
      "15  tatwan_inclass_demo                  \n",
      "16          tatwan_taxi                  \n"
     ]
    }
   ],
   "source": [
    "print(wr.catalog.databases())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943459c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Database</th>\n",
       "      <th>Table</th>\n",
       "      <th>Description</th>\n",
       "      <th>TableType</th>\n",
       "      <th>Columns</th>\n",
       "      <th>Partitions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Database, Table, Description, TableType, Columns, Partitions]\n",
       "Index: []"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wr.catalog.tables(database=database_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086f5abc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'paths': ['s3://techcatalyst-raw/BLAKE_wr/2cbc317b36f04087a5760fe495f9c4e1.snappy.parquet'],\n",
       " 'partitions_values': {}}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wr.s3.to_parquet(\n",
    "    df=df, # the DataFrame you just created \n",
    "    path=f\"s3://techcatalyst-raw/{name}/\", # write to the techcatalyst-raw bucket under your folder name (or it would create a new folder if it does not exist)\n",
    "    dataset=True, \n",
    "    database=database_name, # the name of the database you just created in AWS Glue \n",
    "    table= \"BLAKE_STOCK\", # pick a table name for example YOURNAME_STOCK\n",
    "    mode='overwrite'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72445ea5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Database</th>\n",
       "      <th>Table</th>\n",
       "      <th>Description</th>\n",
       "      <th>TableType</th>\n",
       "      <th>Columns</th>\n",
       "      <th>Partitions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>blake_wr_db</td>\n",
       "      <td>blake_stock</td>\n",
       "      <td></td>\n",
       "      <td>EXTERNAL_TABLE</td>\n",
       "      <td>date, open, high, low, close, volume</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Database        Table Description       TableType  \\\n",
       "0  blake_wr_db  blake_stock              EXTERNAL_TABLE   \n",
       "\n",
       "                                Columns Partitions  \n",
       "0  date, open, high, low, close, volume             "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wr.catalog.tables(name_contains=\"blake\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20ac9fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Database</th>\n",
       "      <th>Table</th>\n",
       "      <th>Description</th>\n",
       "      <th>TableType</th>\n",
       "      <th>Columns</th>\n",
       "      <th>Partitions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>blake_wr_db</td>\n",
       "      <td>blake_stock</td>\n",
       "      <td></td>\n",
       "      <td>EXTERNAL_TABLE</td>\n",
       "      <td>date, open, high, low, close, volume</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>emma_db</td>\n",
       "      <td>emma_stock</td>\n",
       "      <td></td>\n",
       "      <td>EXTERNAL_TABLE</td>\n",
       "      <td>date, open, high, low, close, volume</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>jadenastle_db</td>\n",
       "      <td>jaden_stock</td>\n",
       "      <td>This is my stock table.</td>\n",
       "      <td>EXTERNAL_TABLE</td>\n",
       "      <td>date, open, high, low, close, volume</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>michael_db</td>\n",
       "      <td>michael_stock</td>\n",
       "      <td></td>\n",
       "      <td>EXTERNAL_TABLE</td>\n",
       "      <td>date, open, high, low, close, volume</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tatwan_db</td>\n",
       "      <td>tatwan_stock</td>\n",
       "      <td>This is my stock table.</td>\n",
       "      <td>EXTERNAL_TABLE</td>\n",
       "      <td>date, open, high, low, close, volume</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>tatwan_inclass_demo</td>\n",
       "      <td>tatwan_goog_stock</td>\n",
       "      <td></td>\n",
       "      <td>EXTERNAL_TABLE</td>\n",
       "      <td>date, open, high, low, close, volume</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Database              Table              Description  \\\n",
       "0          blake_wr_db        blake_stock                            \n",
       "1              emma_db         emma_stock                            \n",
       "2        jadenastle_db        jaden_stock  This is my stock table.   \n",
       "3           michael_db      michael_stock                            \n",
       "4            tatwan_db       tatwan_stock  This is my stock table.   \n",
       "5  tatwan_inclass_demo  tatwan_goog_stock                            \n",
       "\n",
       "        TableType                               Columns Partitions  \n",
       "0  EXTERNAL_TABLE  date, open, high, low, close, volume             \n",
       "1  EXTERNAL_TABLE  date, open, high, low, close, volume             \n",
       "2  EXTERNAL_TABLE  date, open, high, low, close, volume             \n",
       "3  EXTERNAL_TABLE  date, open, high, low, close, volume             \n",
       "4  EXTERNAL_TABLE  date, open, high, low, close, volume             \n",
       "5  EXTERNAL_TABLE  date, open, high, low, close, volume             "
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wr.catalog.tables(name_contains=\"stock\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6988431",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1/2/2025 16:00:00</td>\n",
       "      <td>191.49</td>\n",
       "      <td>193.20</td>\n",
       "      <td>188.71</td>\n",
       "      <td>190.63</td>\n",
       "      <td>17545162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1/3/2025 16:00:00</td>\n",
       "      <td>192.73</td>\n",
       "      <td>194.50</td>\n",
       "      <td>191.35</td>\n",
       "      <td>193.13</td>\n",
       "      <td>12874957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1/6/2025 16:00:00</td>\n",
       "      <td>195.15</td>\n",
       "      <td>199.56</td>\n",
       "      <td>195.06</td>\n",
       "      <td>197.96</td>\n",
       "      <td>19483323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1/7/2025 16:00:00</td>\n",
       "      <td>198.27</td>\n",
       "      <td>202.14</td>\n",
       "      <td>195.94</td>\n",
       "      <td>196.71</td>\n",
       "      <td>16966760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1/8/2025 16:00:00</td>\n",
       "      <td>193.95</td>\n",
       "      <td>197.64</td>\n",
       "      <td>193.75</td>\n",
       "      <td>195.39</td>\n",
       "      <td>14335341</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                date    open    high     low   close    volume\n",
       "0  1/2/2025 16:00:00  191.49  193.20  188.71  190.63  17545162\n",
       "1  1/3/2025 16:00:00  192.73  194.50  191.35  193.13  12874957\n",
       "2  1/6/2025 16:00:00  195.15  199.56  195.06  197.96  19483323\n",
       "3  1/7/2025 16:00:00  198.27  202.14  195.94  196.71  16966760\n",
       "4  1/8/2025 16:00:00  193.95  197.64  193.75  195.39  14335341"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = wr.s3.read_parquet_table(database=\"blake_wr_db\", \n",
    "                              table=\"blake_stock\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f14d7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'date': 'string',\n",
       " 'open': 'double',\n",
       " 'high': 'double',\n",
       " 'low': 'double',\n",
       " 'close': 'double',\n",
       " 'volume': 'bigint'}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wr.catalog.get_table_types(database=\"blake_wr_db\", \n",
    "                              table=\"blake_stock\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a87fb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Name': 'blake_stock',\n",
       " 'DatabaseName': 'blake_wr_db',\n",
       " 'CreateTime': datetime.datetime(2025, 8, 4, 19, 30, 41, tzinfo=tzlocal()),\n",
       " 'UpdateTime': datetime.datetime(2025, 8, 4, 19, 30, 41, tzinfo=tzlocal()),\n",
       " 'Retention': 0,\n",
       " 'StorageDescriptor': {'Columns': [{'Name': 'date', 'Type': 'string'},\n",
       "   {'Name': 'open', 'Type': 'double'},\n",
       "   {'Name': 'high', 'Type': 'double'},\n",
       "   {'Name': 'low', 'Type': 'double'},\n",
       "   {'Name': 'close', 'Type': 'double'},\n",
       "   {'Name': 'volume', 'Type': 'bigint'}],\n",
       "  'Location': 's3://techcatalyst-raw/BLAKE_wr/',\n",
       "  'InputFormat': 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat',\n",
       "  'OutputFormat': 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat',\n",
       "  'Compressed': True,\n",
       "  'NumberOfBuckets': -1,\n",
       "  'SerdeInfo': {'SerializationLibrary': 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe',\n",
       "   'Parameters': {'serialization.format': '1'}},\n",
       "  'BucketColumns': [],\n",
       "  'SortColumns': [],\n",
       "  'Parameters': {'CrawlerSchemaDeserializerVersion': '1.0',\n",
       "   'compressionType': 'snappy',\n",
       "   'classification': 'parquet',\n",
       "   'typeOfData': 'file'},\n",
       "  'StoredAsSubDirectories': False},\n",
       " 'PartitionKeys': [],\n",
       " 'TableType': 'EXTERNAL_TABLE',\n",
       " 'Parameters': {'compressionType': 'snappy',\n",
       "  'classification': 'parquet',\n",
       "  'projection.enabled': 'false',\n",
       "  'typeOfData': 'file'},\n",
       " 'CreatedBy': 'arn:aws:iam::535146832369:user/Blake.Liu',\n",
       " 'IsRegisteredWithLakeFormation': False,\n",
       " 'CatalogId': '535146832369',\n",
       " 'VersionId': '0',\n",
       " 'IsMultiDialectView': False}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_details = wr.catalog.get_tables(database=\"blake_wr_db\")\n",
    "\n",
    "next(table_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c367f77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'paths': ['s3://techcatalyst-raw/BLAKE_wr/9288e6c1eed4476d98eade7875cbf9c0.snappy.parquet'],\n",
       " 'partitions_values': {}}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "desc = \"This is my stock table.\"\n",
    "param = {\"source\": \"Google\", \"class\": \"stock\"}\n",
    "comments = {\n",
    "    \"Date\": \"Trading Date\",\n",
    "    \"Open\": \"Opening Price\",\n",
    "    \"Close\": \"Closing Price\"\n",
    "}\n",
    "\n",
    "wr.s3.to_parquet(\n",
    "    df=df,\n",
    "    path=\"s3://techcatalyst-raw/BLAKE_wr\", # CHANGE THIS TO USE YOUR NAME for example s3://TECHCATALYST/TATWAN\n",
    "    dataset=True,\n",
    "    database='blake_wr_db',\n",
    "    table=\"blake_stock\",\n",
    "    mode='overwrite',\n",
    "    glue_table_settings=wr.typing.GlueTableSettings(description=desc,  # here we are passing some metadata\n",
    "                                                    parameters=param, \n",
    "                                                    columns_comments=comments),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af7eba5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Column Name</th>\n",
       "      <th>Type</th>\n",
       "      <th>Partition</th>\n",
       "      <th>Comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>date</td>\n",
       "      <td>string</td>\n",
       "      <td>False</td>\n",
       "      <td>Trading Date</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>open</td>\n",
       "      <td>double</td>\n",
       "      <td>False</td>\n",
       "      <td>Opening Price</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>high</td>\n",
       "      <td>double</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>low</td>\n",
       "      <td>double</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>close</td>\n",
       "      <td>double</td>\n",
       "      <td>False</td>\n",
       "      <td>Closing Price</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>volume</td>\n",
       "      <td>bigint</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Column Name    Type  Partition        Comment\n",
       "0        date  string      False   Trading Date\n",
       "1        open  double      False  Opening Price\n",
       "2        high  double      False               \n",
       "3         low  double      False               \n",
       "4       close  double      False  Closing Price\n",
       "5      volume  bigint      False               "
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wr.catalog.table(database='blake_wr_db', table='blake_stock')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d10a81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['s3://techcatalyst-raw/stage/yellow_tripdata.csv',\n",
       " 's3://techcatalyst-raw/stage/yellow_tripdata.json',\n",
       " 's3://techcatalyst-raw/stage/yellow_tripdata.parquet']"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wr.s3.list_objects('s3://techcatalyst-raw/stage/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92becd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "wr.s3.download(path='s3://techcatalyst-raw/stocks/GOOG.csv', \n",
    "               local_file='./new_file.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1830de",
   "metadata": {},
   "outputs": [],
   "source": [
    "your_name = 'BLAKE_wr'\n",
    "file_name = 'wr_newfile.csv'\n",
    "wr.s3.upload(local_file='new_file.csv',path= f's3://techcatalyst-raw/{your_name}/uploads/{file_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d10ce9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['s3://techcatalyst-raw/BLAKE_wr/uploads/wr_newfile.csv']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wr.s3.list_objects(f's3://techcatalyst-raw/{your_name}/uploads/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b6751b",
   "metadata": {},
   "source": [
    "# Glue Catalog and Athena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de25dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A name for the new Glue Database it should be YOURNAME_TAXI\n",
    "db_name =  \"BLAKE_TAXI\"\n",
    "\n",
    "# A name for the table it should be YOURNAME_TRIPDATA\n",
    "table_name =  \"BLAKE_TRIPDATA\"\n",
    "\n",
    "# the path_direcoty to should point to the bucket (main directory)\n",
    "s3_path_directory = 's3://techcatalyst-raw/taxi_data'\n",
    "\n",
    "# The full path to the actual file (e.g., a CSV or Parquet file)\n",
    "s3_path_file = 's3://techcatalyst-raw/taxi_data/yellow_tripdata_2024-01.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df20022",
   "metadata": {},
   "outputs": [
    {
     "ename": "AlreadyExists",
     "evalue": "Database BLAKE_TAXI already exists and <exist_ok> is set to False.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAlreadyExists\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[120]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m wr.catalog.delete_table_if_exists(database=db_name, table=table_name) \n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Create the new Glue database first based on the db_name you created\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43mwr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcatalog\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_database\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdb_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# This function can extract the schema from our file and returns a tuple: (schema, partitions). We only need the schema. \u001b[39;00m\n\u001b[32m      8\u001b[39m columns_types, partitions_types = wr.s3.read_parquet_metadata(path=s3_path_file)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/awswrangler/_config.py:712\u001b[39m, in \u001b[36mapply_configs.<locals>.wrapper\u001b[39m\u001b[34m(*args_raw, **kwargs)\u001b[39m\n\u001b[32m    710\u001b[39m         \u001b[38;5;28;01mdel\u001b[39;00m args[name]\n\u001b[32m    711\u001b[39m         args = {**args, **keywords}\n\u001b[32m--> \u001b[39m\u001b[32m712\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/awswrangler/catalog/_create.py:712\u001b[39m, in \u001b[36mcreate_database\u001b[39m\u001b[34m(name, description, catalog_id, exist_ok, database_input_args, boto3_session)\u001b[39m\n\u001b[32m    710\u001b[39m r = client_glue.get_database(**_catalog_id(catalog_id=catalog_id, Name=name))\n\u001b[32m    711\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m exist_ok:\n\u001b[32m--> \u001b[39m\u001b[32m712\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions.AlreadyExists(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDatabase \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m already exists and <exist_ok> is set to False.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    713\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m args.items():\n\u001b[32m    714\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m v != r[\u001b[33m\"\u001b[39m\u001b[33mDatabase\u001b[39m\u001b[33m\"\u001b[39m].get(k, \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[31mAlreadyExists\u001b[39m: Database BLAKE_TAXI already exists and <exist_ok> is set to False."
     ]
    }
   ],
   "source": [
    "# uncomment below if you ran into issues to clean things up and rerun the cell\n",
    "wr.catalog.delete_table_if_exists(database=db_name, table=table_name) \n",
    "\n",
    "# Create the new Glue database first based on the db_name you created\n",
    "wr.catalog.create_database(name=db_name)\n",
    "\n",
    "# This function can extract the schema from our file and returns a tuple: (schema, partitions). We only need the schema. \n",
    "columns_types, partitions_types = wr.s3.read_parquet_metadata(path=s3_path_file)\n",
    "print(\"Successfully read schema from Parquet file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396c1570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully read schema from Parquet file.\n"
     ]
    }
   ],
   "source": [
    "columns_types, partitions_types = wr.s3.read_parquet_metadata(path=s3_path_file)\n",
    "print(\"Successfully read schema from Parquet file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d08f35e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 'BLAKE_TRIPDATA' created successfully in database 'BLAKE_TAXI'.\n"
     ]
    }
   ],
   "source": [
    "wr.catalog.create_parquet_table(\n",
    "    database=db_name, # pass the database name\n",
    "    table=table_name, # pass the table name\n",
    "    path=s3_path_directory, # use the directoy here \n",
    "    columns_types=columns_types,  # Pass the schema here\n",
    "    partitions_types=partitions_types\n",
    ")\n",
    "print(f\"Table '{table_name}' created successfully in database '{db_name}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acce6305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query Results:\n",
      "   vendorid tpep_pickup_datetime tpep_dropoff_datetime  passenger_count  \\\n",
      "0         2  2024-04-03 17:49:47   2024-04-03 18:39:29              NaN   \n",
      "1         2  2024-04-03 17:58:05   2024-04-03 18:04:41              NaN   \n",
      "2         2  2024-04-03 17:27:15   2024-04-03 17:58:46              NaN   \n",
      "3         2  2024-04-03 17:01:56   2024-04-03 17:31:43              NaN   \n",
      "4         2  2024-04-03 17:42:09   2024-04-03 17:42:57              NaN   \n",
      "\n",
      "   trip_distance  ratecodeid store_and_fwd_flag  pulocationid  dolocationid  \\\n",
      "0           3.77         NaN               <NA>           236           234   \n",
      "1           0.70         NaN               <NA>           233           233   \n",
      "2           4.63         NaN               <NA>           237           246   \n",
      "3           3.32         NaN               <NA>            87           186   \n",
      "4           0.00         NaN               <NA>           246           246   \n",
      "\n",
      "   payment_type  fare_amount  extra  mta_tax  tip_amount  tolls_amount  \\\n",
      "0             0        35.71    0.0      0.5        0.00           0.0   \n",
      "1             0        14.22    0.0      0.5        2.73           0.0   \n",
      "2             0        31.38    0.0      0.5        0.00           0.0   \n",
      "3             0        35.56    0.0      0.5        0.00           0.0   \n",
      "4             0        14.22    0.0      0.5        3.64           0.0   \n",
      "\n",
      "   improvement_surcharge  total_amount  congestion_surcharge  airport_fee  \n",
      "0                    1.0         39.71                   NaN          NaN  \n",
      "1                    1.0         20.95                   NaN          NaN  \n",
      "2                    1.0         35.38                   NaN          NaN  \n",
      "3                    1.0         39.56                   NaN          NaN  \n",
      "4                    1.0         21.86                   NaN          NaN  \n"
     ]
    }
   ],
   "source": [
    "query = f\"SELECT * FROM {table_name} LIMIT 5\"\n",
    "\n",
    "df = wr.athena.read_sql_query(query, database=db_name)\n",
    "\n",
    "print(\"\\nQuery Results:\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302e46ea",
   "metadata": {},
   "source": [
    "# Lab 4 - Lambda function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5711ec1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client.upload_file(Filename='GOOG.csv',\n",
    "                      Bucket='techcatalyst-raw',\n",
    "                      Key='BLAKE_wr/GOOG.csv'\n",
    "                      )\n",
    "                      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a0c9ee0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client.upload_file(Filename='test.csv',\n",
    "                      Bucket='techcatalyst-raw',\n",
    "                      Key='BLAKE/test.csv'\n",
    "                      )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
